# 人工智能视觉方向第三轮考核文档
## 前言
本轮是视觉方向的最后一轮考核，本轮的主要内容是实现目标检测。
## 考核任务
* 模型需要使用类Transformer机制或FCN机制，可以参考Vit模型和yolo模型
* 不允许使用任何从诸如torchvision这种地方导出来的模型，更不允许pretrained=True
* 使用Pascal VOC 2012 数据集，提供一个下载数据集的[镜像地址](https://pjreddie.com/projects/pascal-voc-dataset-mirror/)
* 向Pascal VOC数据集官方提交一份验证数据，提交地址在[这里](http://host.robots.ox.ac.uk:8080/)
## 上一轮存在的问题
非常严重的问题是使用GRU/LSTM的方式，众所周知这种模型的输入是(batch_size,step,feature)的形状，而图像的一般是(batch_size,x,y)，如果是彩色图则是(batch_size,x,y,channal),上一轮中大家都直接将(x,y)等同于(step,feature)直接输入模型，例如一张1920*1080的图片，将其拆分成一个长度为序列1920、每个step有1080维特征，然后直接输入模型。

究其底层原理，以GRU为例，以上文所诉的方法进行计算时，位于第一个step处的gru单元永远只能看到第一列的像素，第n个gru单元只能看到0~n这几列的像素，只有位于最后一个step处的gru单元才可见全图，显然这是人为的缩小的整个模型的视野，降低了空间的使用率。其次这种方法只适用于灰度图，彩图上一轮md还有人直接用view()强行降维然后输入lstm网络，看了有效提升血压。

解决这个问题的办法也有，对于第一个问题，可以使用例如双向RNN模型，但是双向rnn也有rnn固有的缺点例如对于长序列的感知力很低，放在图片上这个问题更加突出了，其次可以使用attention机制，使用Transformer架构，Transformer架构已经被证明是在长序列预测上效果显著的sota模型；结合第二个问题，为了契合此类序列模型的输入，固然要对输入进行降维，但是降维的方式应当是将(x,y)这个维度进行合并，要将一张二维图片降维成一维序列，有很多方式，可以简单的将图像像贪吃蛇一样抽丝，或者高端一点使用希尔伯特曲线对图像进行遍历，又或者使用Vit使用的方法用LinerProjection进行变换，总之是目的是使模型的每一组权重参数都能尽可能多的看到信息，以此提升效率。

## 算力补贴
为了防止各位爆显存，通过第二轮的同学本轮每人可有100r上限的服务器租用补贴。服务器租用的教程可以看docs文件夹。

## 附加信息
1. 希望各位可以多看看英文论文以及多交流问题
2. 可以大胆创新新的底层架构，但是最好言之有理，能说得通想要模型有什么表现，为了产生这种表现设计了何种架构，这种架构为何能更好实现这种表现，写模型的时候多问问自己这个问题
3. 因为有点难，本次考核时长3周，每隔约一周会有一次交流会，届时有问题可以一起讨论

## 考核提交
1. GitHub代码链接
2. train和test数据集上的正确率、交并比随epoch的变化曲线图，图例图名等等该有的都画上，要让人一目了然这张图说明了什么。
3. 学习笔记，上次各位的学习笔记还是太少了，可以放在一个文件夹下，不同主题的笔记放不同文件下。
