# Task 8：CV-Apply

## **学习目的：从模型调用到应用大师**

你已经在 Task 7 中成功驾驭了 `timm`、`opencv`、`Albumentations` 和 `OpenMMLab` 这些强大的 CV 生态工具。你不再是一个只会 `imread` 和 `imshow` 的入门者，而是已经能熟练加载预训练模型、应用数据增强、并使用工业级框架进行推理的实战者。

然而，真正的挑战才刚刚开始。Task 7 的原型，如同在靶场里试射的神器，虽然威力无穷，却从未见过真正的战场。它们在 COCO、ImageNet 等标准数据集上表现优异，但无法解决你**特定场景**下的**特定问题**（比如识别校园里的电瓶车，或是检测生产线上的瑕疵）；它们能输出结果，却未经一个**完整的、脏活累活的数据闭环**所淬炼。

**Task 8，正是你从模型使用者向问题解决者蜕变，为你神器注入实战之魂的时刻！**

此阶段的学习目的在于：

* **深入 SOTA 模型的心脏**：你将不再满足于调用 `timm.create_model`。本轮，你将深入掌握工业界应用最广泛的 **YOLO 系列（目标检测）** 与 **R-CNN 家族（分割/检测）**。你将亲手训练它们，让它们认识你独有的、全新的物体。
* **掌握数据->标注->训练->评估的全链路**：这是本轮最核心的挑战！你将亲手打通一个CV应用的完整生命周期：从 **Task 2** 的爬虫技能（或自己拍摄）出发，**自主收集**特定场景的图像；学会使用**标注工具**，亲手告诉模型什么是对的；运用 **Task 7** 的增强与框架技术，完成一次**自定义数据集**上的完整训练与评估。
* **从单一脚本到系统工程的跨越**：你不再是构建零散的测试脚本，而是要完成一个完整的感知系统。这个项目要求你像一名真正的CV工程师一样思考，如何定义问题、收集和处理脏数据、选择并调优模型、最后用客观的指标去衡量它的成败。

完成本轮试炼，你将不再是demo的运行者，而是真正具备了从0到1，构建一个解决特定视觉问题的、高性能、定制化CV应用的综合能力！

---

## 学习内容：高级CV应用开发

本阶段将聚焦于将 Task 7 的基础组件，升级和融合成一个复杂、完整的项目。

* **自定义数据集构建 (The Foundation)**

  * **数据收集**：从网络（Task 2）、公开数据集或自己拍摄等渠道，获取针对特定任务的原始图像。
  * **数据标注**：**重中之重**！学习使用专业的标注工具（如 `LabelImg`, `LabelMe`, `CVAT`），为你的图像数据制作**目标检测（Bounding Box）**或**实例分割（Polygon）**的标签。
  * **数据格式转换**：学习如何将你的标注数据转换为常用框架所需的格式（如 `COCO JSON` 或 `YOLO TXT`）。
* **SOTA模型微调实战 (SOTA Fine-tuning)**

* **工业级框架深入**：深入 `OpenMMLab`（如 `MMDetection`）或 `Ultralytics (YOLO)` 等主流框架，学习它们独特的**配置文件驱动（Config-driven）** 的训练方式。
  * **模型选型与训练**：根据你的任务需求（速度 vs 精度），选择合适的模型（如 `YOLOv8`、`Faster R-CNN`、`Mask R-CNN`），加载预训练权重，并在你的**自定义数据集**上进行微调。
  * **关键指标解读**：深刻理解衡量模型性能的核心指标，如 **`mAP`（mean Average Precision）** 用于目标检测，**`mIoU`（mean Intersection over Union）** 用于分割。
* **高级数据增强（Advanced Augmentation）**

  * 深入 `Albumentations`，学习如何构建一个强大的、适用于你特定场景的**数据增强流水线**（如 `Compose`），应用 `Mosaic`、`MixUp` 等高级增强策略，有效抑制过拟合。
* **模型分析与可视化（Analysis & Visualization）**

  * 学习如何将模型的预测结果（检测框、分割蒙版）**可视化**到原始图片上。
  * 分析模型的**失败案例（Failure Cases）**，并思考可能的原因与改进方向（是数据问题？还是模型问题？）。

---

## 推荐教程：

* **数据标注（Annotation）**

  * [LabelImg（目标检测标注）](https://github.com/HumanSignal/labelImg)（简单易上手的经典工具）
  * [LabelMe（分割/多边形标注）](https://github.com/HumanSignal/labelme)
  * [CVAT.ai（强大的开源在线标注平台）](https://www.cvat.ai/)
* **模型训练框架（Training Frameworks）**

  * [Ultralytics YOLOv8 官方文档](https://docs.ultralytics.com/)（**必读**，训练自定义数据的最佳入门）
  * [OpenMMLab MMDetection 官方文档](https://mmdetection.readthedocs.io/en/latest/)（工业界最流行的框架之一，专业且强大）
  * [B 站/知乎搜索 YOLOv8/MMDetection 训练自定义数据集](https://www.google.com/search?q=YOLOv8+%E8%AE%AD%E7%BB%83%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86)（寻找端到端的项目教程）

---

## 作业：构建你的专属魔法眼（All-in-One Project）

本轮作业的重点是 **项目完整性** 和 **解决真实问题**。你需要将 task1-7 学到的技能整合成一个单一的、强大的应用。我们将不再提供多个小作业，而是只提供一个综合性的大项目。

比如说，你可以做一个校园流浪猫检测器，或者一个课堂举手姿态识别器，甚至是你专业领域的瑕疵检测器。

### **作业要求（All-in-One）**

#### **阶段一：数据集的从无到有（标注）**

1. **定义你的问题**

   * 请自行选择**一个你感兴趣的、有价值的视觉任务**（例如：识别特定种类的花卉、检测教室里空余的座位、识别围棋棋盘上的棋子等）。
2. **收集并标注数据**

   * 收集至少 **100-200 张**相关的图片。
   * 使用 `LabelImg`（检测）或 `LabelMe`（分割）等工具，**亲手完成**这些数据的标注工作，并划分为训练集和验证集。

#### **阶段二：模型的训练与调优（Task 8 技能）**

1. **选择框架与模型**

   * 选择一个主流框架（推荐 `YOLOv8` 或 `MMDetection`）和一个 SOTA 模型。
2. **配置与训练**

   * 将你在【阶段一】制作的数据集，转换为框架要求的格式。
   * 通过**修改配置文件**，配置好你的数据集路径、模型参数、训练策略（如学习率、迭代次数），并**启动训练**。
3. **【高级要求】**：你必须在训练过程中，至少实现以下一项**调优或对比实验**：

   * **优化 A（数据增强）**：设计两套不同的数据增强策略（一套简单，一套复杂），对比它们对最终模型性能（`mAP` / `mIoU`）的影响。
   * **优化 B（模型对比）**：使用完全相同的数据和训练配置，训练两个不同的模型（如 `YOLOv8s` vs `YOLOv8m`，或 `Faster R-CNN` vs `RetinaNet`），并对比分析它们的性能差异。

#### **阶段三：评估、分析与可视化**

1. **评估模型**

   * 在验证集上运行评估脚本，得到并报告你模型的核心性能指标（`mAP` / `mIoU`）。
2. **可视化结果**

   * 编写脚本，将你的模型应用在一些**测试图片（从未用于训练或验证）**上，并将检测框或分割蒙版画在图片上进行展示。

#### **阶段四：报告与演示**

1. 以 **Jupyter Notebook** 或 Markdown 形式提交你的代码和分析报告。
2. 在报告中，请详细说明：

   * 你的项目设计（要解决什么问题）、数据收集与标注流程。
   * 你的模型训练配置（使用了哪个模型、关键参数是什么）。
   * 你的【高级要求】实验过程与结果对比分析。
   * 最终模型的性能指标，以及你对这个结果的看法。
3. **【最终演示】**：

   * 请在报告中，**必须**展示至少 **5 张** 测试图片的**可视化结果**。这些结果需要能清晰地展示出你的模型在不同场景下的表现（**需要包含成功和失败的案例**），并对失败的案例进行简要的**原因分析**。
