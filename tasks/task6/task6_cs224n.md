## Task 6

## **学习目的**

恭喜你，坚持到这里的勇士。你已经走过了漫长而艰难的道路：在Task 4中，你亲手铸造了反向传播的引擎；在Task 5中，你用模块化的思想构建起了现代卷积神经网络的骨架。你不再是一个只能调用API的旁观者，而是一个真正理解神经网络内部运作机制的构建者。

现在，欢迎来到NLP学习之旅的终章，也是你基础阶段的“毕业之战”。

在本轮考核中，我们将把目光从相对基础的文本分类、序列标注任务中移开，踏入更广阔、更深刻的自然语言 **“理解”** 与 **“生成”** 之境。你将探索语言是如何被赋予数学形态的，机器是如何解析复杂句法结构的，甚至是如何像一位翻译家一样，在不同语言之间架起沟通的桥梁。

这次任务将是你从理解经典模型到接触前沿研究的桥梁。完成它，意味着你将具备初步阅读NLP顶会论文、理解并实现复杂语言系统的核心能力。

## **学习内容**

本轮你将接触到一系列定义了现代NLP研究方向的里程碑式模型与思想：

* **词的向量表示**: 探索 `word2vec` 的奥秘，理解计算机如何通过向量运算捕捉词语间的语义关系。
* **句法结构分析**: 学习并实现一个基于神经网络的 **依存句法分析器**，让模型看穿句子的骨架。
* **序列到序列模型**: 构建一个带 **注意力机制 (Attention)** 的 **神经机器翻译 (NMT)** 系统，体验模型在“翻译”时的专注与思考。
* **Transformer架构**: 深入理解驱动了当今大语言模型革命的核心——**自注意力机制 (Self-Attention)**，并亲手实现它的关键组件。

## **学习要求**

与前几次作业相比，本轮任务对 **数学功底、理论深度和代码实现精度** 的要求达到了顶峰。

1. **深入理论腹地**: CS224n以其理论深度著称。你需要花费大量时间消化课程笔记，彻底理解反向传播在RNN（BPTT）、Attention和Transformer中的推导细节。
2. **代码精确实现**: 本次作业中的很多模块（如NMT的`forward`和`backward`）对代码的精确度要求极高，一个微小的索引错误或维度不匹配都可能导致整个系统崩溃。
3. **系统级整合与调试**: 你需要将编码器、解码器、注意力模块等多个复杂组件严丝合缝地拼接起来。调试这样一个庞大的系统极具挑战，你需要更有耐心，并学会如何设计单元测试来验证每个模块的正确性。

## **作业**

**核心任务：完成 [CS224n 四大编程作业](https://web.stanford.edu/class/cs224n/)**

本次考核将由四个核心模块构成，它们将引领你一步步攀上NLP领域的技术之巅。

### **第一部分：词向量的探索与应用 (Assignment 1)**

在这一部分，你将深入探索词语在向量空间中的神奇表示。你需要：

* 理解`word2vec`模型（CBOW和Skip-Gram）背后的核心思想。
* 使用`gensim`等工具进行实践，并完成基于词向量的语义类比测试（如 "king - man + woman ≈ queen"），直观感受词向量的魅力。

### **第二部分：神经网络与依存句法分析 (Assignment 2)**

在理解了词的表示后，你将开始让机器去理解句子的结构。你需要：

* 实现一个基于神经网络的依存句法分析器。
* 深刻理解模型是如何通过分析句子中词语之间的修饰与被修饰关系，来形成对整个句子结构的“语法树”的。

### **第三部分：神经机器翻译 (Assignment 3)**

这是最激动人心的挑战之一！你将构建一个能将一种语言翻译成另一种语言的系统。你需要：

* 实现一个带有**注意力机制 (Attention)** 的**编码器-解码器 (Encoder-Decoder)** 模型。
* 在训练过程中，你将能亲眼看到，模型在生成每一个目标语言单词时，它的“注意力”是如何在源语言句子的不同部分上动态聚焦的。

### **第四部分：探秘Transformer (Assignment 4)**

在体验了RNN处理序列的模式后，你将接触到当今AI世界真正的“版本答案”——Transformer。你需要：

* 学习并亲手实现**自注意力机制 (Self-Attention)** 和**多头注意力 (Multi-Head Attention)** 的核心计算过程。
* 通过构建一个简化的Transformer模型，你将彻底搞懂它为何能并行处理序列，并成为GPT等所有大语言模型的基石。

---

## **⭐ 重要提示 ⭐**

* **关于最终项目 (Final Project)**：为了让大家能将精力完全集中在对核心知识的编码实现上，本次考核**无需完成**CS224n的最终项目 (Final Project) 哦！
* **关于翻译与学习资料**：在学习过程中，如果遇到困难，或想为社区做出贡献，强烈推荐参考（和参与）我们自己的学习仓库：

  * **[ShaddockNH3/CS224N-Nyan-Book](https://github.com/ShaddockNH3/CS224N-Nyan-Book)**
  * 这是本人的学习仓库，你可以在这里找到课程PPT的翻译、论文精读笔记等宝贵资料，尤其是环境配置（cs224n的环境配置没有cs231n配置那么方便）。仓库下放着本人的实现代码，这部分请不要看。

### **作业参考资料**

1. [跟李沐学AI 词向量（word2vec）【动手学深度学习v2】](https://www.bilibili.com/video/BV1sY4y1572C/)
2. [跟李沐学AI 注意力机制【动手学深度学习v2】](https://www.bilibili.com/video/BV1ui4y1j783/)
3. [跟李沐学AI Transformer论文逐段精读【论文精读】](https://www.bilibili.com/video/BV1pu411o7BE/)
4. [【Transformer 其实是个简单到令人困惑的模型【白话DeepSeek-06】】](https://www.bilibili.com/video/BV1C3dqYxE3q/)
5. [台大李宏毅老师 机器学习2021 (Self-Attention和Transformer部分)](https://www.bilibili.com/video/BV1JA411X76s?p=65)

### **作业要求**

1. **严禁抄袭**。这是你的毕业之战，请用一份独立完成的作品为你这段旅程画上圆满的句号，这既是对知识的尊重，也是对自己的负责。
2. **善用工具**。遇到问题时，首先尝试通过搜索引擎、官方文档和AI助手解决。如果仍然无法解决，欢迎在群里进行有深度的、描述清晰的提问。
3. **拥抱AI，但保持思考**。不限制使用ChatGPT等大语言模型工具辅助学习和Debug，但你必须确保能完全理解模型生成的每一行代码的含义。
4. **规范化提交**。所有作业均需通过Git提交到你个人的GitHub仓库中。

### **作业提交方式**

1. 在你的个人GitHub仓库中为本次作业创建一个新的文件夹 (例如 `task6-cs224n`)。
2. 将你完成的所有代码（`.py`和`.ipynb`文件）上传到该文件夹。
3. 通过Pull Request的方式，在课程仓库的 `solutions.md` 文件中更新你的仓库地址。
